{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import *\n",
    "from TimeBasedCV import TimeBasedCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (Temp/ipykernel_9312/297288424.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Student\\AppData\\Local\\Temp/ipykernel_9312/297288424.py\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    for train_index, test_index in tscv.split(transaction, date_column='t_dat', stride=30):\u001b[0m\n\u001b[1;37m                                                                                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "transaction=pd.read_parquet('data\\\\HM_parquet\\\\transactions_train.parquet')\n",
    "tscv = TimeBasedCV(train_period=30,\n",
    "                   test_period=7,\n",
    "                   freq='days')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tscv.get_n_splits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 機器學習範例\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "X = transaction[['record_date',columns]]\n",
    "y = transaction[label]\n",
    "\n",
    "scores = []\n",
    "for train_index, test_index in tscv.split(X, date_column='t_dat', stride=30):\n",
    "\n",
    "    data_train   = X.loc[train_index].drop('record_date', axis=1)\n",
    "    target_train = y.loc[train_index]\n",
    "\n",
    "    data_test    = X.loc[test_index].drop('record_date', axis=1)\n",
    "    target_test  = y.loc[test_index]\n",
    "\n",
    "    # if needed, do preprocessing here\n",
    "\n",
    "    clf = LinearRegression()\n",
    "    clf.fit(data_train,target_train)\n",
    "\n",
    "    preds = clf.predict(data_test)\n",
    "\n",
    "    # accuracy for the current fold only    \n",
    "    r2score = clf.score(data_test,target_test)\n",
    "\n",
    "    scores.append(r2score)\n",
    "\n",
    "# this is the average accuracy over all folds\n",
    "average_r2score = np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# implicit.ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os.environ['OPENBLAS_NUM_THREADS']='1'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import implicit\n",
    "from scipy.sparse import coo_matrix\n",
    "from implicit.evaluation import mean_average_precision_at_k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfu = pd.read_parquet('data/HM_parquet/customers.parquet')\n",
    "dfi = pd.read_parquet('data/HM_parquet/articles.parquet')\n",
    "ALL_USERS = dfu['customer_id'].unique().tolist()\n",
    "ALL_ITEMS = dfi['article_id'].unique().tolist()\n",
    "\n",
    "user_ids = dict(list(enumerate(ALL_USERS)))\n",
    "item_ids = dict(list(enumerate(ALL_ITEMS)))\n",
    "\n",
    "user_map = {u: uidx for uidx, u in user_ids.items()}\n",
    "item_map = {i: iidx for iidx, i in item_ids.items()}\n",
    "# 將落落長的使用者id和商品id轉為編號\n",
    "transaction['user_id'] = transaction['customer_id'].map(user_map)\n",
    "transaction['item_id'] = transaction['article_id'].map(item_map)\n",
    "\n",
    "del dfu, dfi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "35ca060d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-18T17:29:04.039473Z",
     "iopub.status.busy": "2022-02-18T17:29:04.038608Z",
     "iopub.status.idle": "2022-02-18T17:29:04.040407Z",
     "shell.execute_reply": "2022-02-18T17:29:04.040901Z",
     "shell.execute_reply.started": "2022-02-08T23:11:16.952239Z"
    },
    "papermill": {
     "duration": 0.059458,
     "end_time": "2022-02-18T17:29:04.041032",
     "exception": false,
     "start_time": "2022-02-18T17:29:03.981574",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_user_item_coo(df, user, item):\n",
    "    \"\"\"\n",
    "    Turn a dataframe with transactions into a COO sparse items x users matrix\n",
    "\n",
    "    Args:\n",
    "        df ([pandas.DataFrame])\n",
    "        user ([pandas.Series])\n",
    "        item ([pandas.Series])\n",
    "\n",
    "    Returns:\n",
    "        coo ([]): \n",
    "    \"\"\"    \n",
    "    row = user.values\n",
    "    col = item.values\n",
    "    data = np.ones(df.shape[0])\n",
    "    coo = coo_matrix((data, (row, col)), shape=(len(ALL_USERS), len(ALL_ITEMS)))\n",
    "    return coo\n",
    "\n",
    "def get_val_matrices(data_train, data_val):\n",
    "    \"\"\" Split into training and validation and create various matrices\n",
    "        \n",
    "        Returns a dictionary with the following keys:\n",
    "            coo_train: training data in COO sparse format and as (users x items)\n",
    "            csr_train: training data in CSR sparse format and as (users x items)\n",
    "            csr_val:  validation data in CSR sparse format and as (users x items)\n",
    "    \n",
    "    \"\"\"\n",
    "    coo_train = to_user_item_coo(data_train)\n",
    "    coo_val = to_user_item_coo(data_val)\n",
    "\n",
    "    csr_train = coo_train.tocsr()\n",
    "    csr_val = coo_val.tocsr()\n",
    "    \n",
    "    return {'coo_train': coo_train,\n",
    "            'csr_train': csr_train,\n",
    "            'csr_val': csr_val\n",
    "          }\n",
    "\n",
    "\n",
    "def validate(matrices, factors=200, iterations=20, regularization=0.01, show_progress=True):\n",
    "    \"\"\" Train an ALS model with <<factors>> (embeddings dimension) \n",
    "    for <<iterations>> over matrices and validate with MAP@12\n",
    "    \"\"\"\n",
    "    coo_train, csr_train, csr_val = matrices['coo_train'], matrices['csr_train'], matrices['csr_val']\n",
    "    \n",
    "    model = implicit.als.AlternatingLeastSquares(factors=factors, \n",
    "                                                 iterations=iterations, \n",
    "                                                 regularization=regularization, \n",
    "                                                 random_state=42)\n",
    "    model.fit(coo_train, show_progress=show_progress)\n",
    "    \n",
    "    # The MAPK by implicit doesn't allow to calculate allowing repeated items, which is the case.\n",
    "    # TODO: change MAP@12 to a library that allows repeated items in prediction\n",
    "    map12 = mean_average_precision_at_k(model, csr_train, csr_val, K=12, show_progress=show_progress, num_threads=4)\n",
    "    # print(f\"Factors: {factors:>3} - Iterations: {iterations:>2} - Regularization: {regularization:4.3f} ==> MAP@12: {map12:6.5f}\")\n",
    "    return map12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7441b152",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-18T17:29:04.404903Z",
     "iopub.status.busy": "2022-02-18T17:29:04.404081Z",
     "iopub.status.idle": "2022-02-18T17:34:47.997861Z",
     "shell.execute_reply": "2022-02-18T17:34:47.998298Z"
    },
    "papermill": {
     "duration": 343.644869,
     "end_time": "2022-02-18T17:34:47.998446",
     "exception": false,
     "start_time": "2022-02-18T17:29:04.353577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_ALS(data_train,data_val):\n",
    "    matrices = get_val_matrices(data_train=data_train, data_val=data_val)\n",
    "    best_map12 = 0\n",
    "    best_params = \"\"\n",
    "    for factors in [40, 50, 60, 100, 200, 500, 1000]:\n",
    "        for iterations in [3, 12, 14, 15, 20]:\n",
    "            for regularization in [0.01]:\n",
    "                map12 = validate(matrices, factors, iterations, regularization, show_progress=False)\n",
    "                if map12 > best_map12:\n",
    "                    best_map12 = map12\n",
    "                    best_params = f\"Best MAP@12 = {best_map12}. factors: {factors}, iterations: {iterations}, regularization: {regularization}\"\n",
    "    print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_map12 = 0\n",
    "best_params = \"\"\n",
    "    for factors in [40, 50, 60, 100, 200, 500, 1000]:\n",
    "        for iterations in [3, 12, 14, 15, 20]:\n",
    "            for regularization in [0.01]:\n",
    "                scores = []\n",
    "                for train_index, val_index in tscv.split(transaction, date_column='t_dat', stride=30):\n",
    "                    \n",
    "                    data_train = transaction.loc[train_index]\n",
    "                    data_val = transaction.loc[val_index]\n",
    "                    matrices = get_val_matrices(data_train, data_val)\n",
    "                    \n",
    "                    \n",
    "\n",
    "                    scores.append(r2score)\n",
    "\n",
    "                # this is the average accuracy over all folds\n",
    "                average_r2score = np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# How to use TimeBasedCV\n",
    "data_for_modeling=pd.read_csv('data.csv', parse_dates=['record_date'])\n",
    "tscv = TimeBasedCV(train_period=30,\n",
    "                   test_period=7,\n",
    "                   freq='days')\n",
    "for train_index, test_index in tscv.split(data_for_modeling,\n",
    "                   validation_split_date=datetime.date(2019,2,1), date_column='record_date'):\n",
    "    print(train_index, test_index)\n",
    "\n",
    "# get number of splits\n",
    "tscv.get_n_splits()\n",
    "\n",
    "#### Example- compute average test sets score: ####\n",
    "X = data_for_modeling[['record_date',columns]]\n",
    "y = data_for_modeling[label]\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "scores = []\n",
    "for train_index, test_index in tscv.split(X, validation_split_date=datetime.date(2019,2,1)):\n",
    "\n",
    "    data_train   = X.loc[train_index].drop('record_date', axis=1)\n",
    "    target_train = y.loc[train_index]\n",
    "\n",
    "    data_test    = X.loc[test_index].drop('record_date', axis=1)\n",
    "    target_test  = y.loc[test_index]\n",
    "\n",
    "    # if needed, do preprocessing here\n",
    "\n",
    "    clf = LinearRegression()\n",
    "    clf.fit(data_train,target_train)\n",
    "\n",
    "    preds = clf.predict(data_test)\n",
    "\n",
    "    # accuracy for the current fold only    \n",
    "    r2score = clf.score(data_test,target_test)\n",
    "\n",
    "    scores.append(r2score)\n",
    "\n",
    "# this is the average accuracy over all folds\n",
    "average_r2score = np.mean(scores)\n",
    "#### End of example ####\n",
    "\n",
    "#### Example- RandomizedSearchCV ####\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from lightgbm import LGBMRegressor\n",
    "from random import randint, uniform\n",
    "\n",
    "tscv = TimeBasedCV(train_period=10, test_period=3)\n",
    "index_output = tscv.split(data_for_modeling, validation_split_date=datetime.date(2019,2,1))\n",
    "\n",
    "lgbm = LGBMRegressor()\n",
    "\n",
    "lgbmPd = {\" max_depth\": [-1,2]\n",
    "         }\n",
    "\n",
    "model = RandomizedSearchCV(\n",
    "    estimator = lgbm,\n",
    "    param_distributions = lgbmPd,\n",
    "    n_iter = 10,\n",
    "    n_jobs = -1,\n",
    "    iid = True,\n",
    "    cv = index_output,\n",
    "    verbose=5,\n",
    "    pre_dispatch='2*n_jobs',\n",
    "    random_state = None,\n",
    "    return_train_score = True)\n",
    "\n",
    "model.fit(X.drop('record_date', axis=1),y)\n",
    "model.cv_results_\n",
    "#### End of example ####\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7a92cbcf0e54319bd537504dd06c788c2290917353461c754c87e8cf50a6053b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('Python38_20211231': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
