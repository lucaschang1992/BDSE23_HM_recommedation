{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ee3d7fd-c0c3-47bf-820b-cb2b94e70908",
   "metadata": {},
   "source": [
    "# 0. 設定 Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "458744cd-e2db-463e-a419-0ad9e6145e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://bdse111.example.com:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>maggie</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f901c114820>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3706663a-2717-490c-8787-adaf1af62898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark.pandas as ps\n",
    "import numpy as np\n",
    "import gc\n",
    "import pyspark.sql.functions as sf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import array_max\n",
    "from pyspark.ml.feature import StringIndexer, IndexToString\n",
    "from pyspark.pandas.config import set_option, reset_option\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e94b47e-b307-4b85-9c1e-85056953d516",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", True)\n",
    "ps.set_option(\"compute.default_index_type\", \"distributed\")\n",
    "set_option(\"compute.ops_on_diff_frames\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b71f5416-468d-48ff-9a59-c12a5ca92041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'512m'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.kryoserializer.buffer.max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "455b950f-bc8d-4ca3-9342-46b61c237c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/hadoop'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.yarn.appMasterEnv.SURPRISE_DATA_FOLDER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a1e3222-a08a-4792-8606-dc36295cdad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/hadoop'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.executorEnv.SURPRISE_DATA_FOLDER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5aeea625-07ec-437d-8d57-7153d5835f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PERIOD = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1211e0ab-6b6c-4e2f-a7f0-48dde3c45262",
   "metadata": {},
   "source": [
    "# 1. 讀取檔案 => tran_ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818ed44a-369c-4a67-a68a-aee7f5bf83ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取檔案\n",
    "# customers = ps.read_parquet('/user/HM_parquet/customers.parquet')\n",
    "# articles = ps.read_parquet('/user/HM_parquet/articles.parquet')\n",
    "tran_ps = ps.read_parquet('/user/HM_parquet/transactions_train.parquet').drop(['price', 'sales_channel_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8881aeab-fa63-4d2f-8f4b-fc371e3ef2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tran_ps.set_index('t_dat',inplace=True)\n",
    "tran_ps['start_test'] = ''\n",
    "tran_ps['split_id'] = ''\n",
    "tran_ps.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b66614e-1b55-43e3-a632-42d05d0be6ef",
   "metadata": {},
   "source": [
    "# 2. 作時間分割 tran_ps => split_data, split_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9079e109-bcd4-4284-a78c-e8f4b6fbcc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(data,train_period=30, test_period=7, stride=30,show_progress=False):\n",
    "    \n",
    "    split_data = ps.DataFrame(columns = ['t_dat','customer_id', 'article_id', 'split_id', 'start_test']).set_index('t_dat',inplace=True)\n",
    "\n",
    "    end_test = data.index.max()\n",
    "    start_test = end_test - relativedelta(days=test_period)\n",
    "    start_train = start_test - relativedelta(days=train_period)\n",
    "    split_id=0\n",
    "\n",
    "    while start_train >= data.index.min():\n",
    "\n",
    "        df = data.loc[start_train:end_test]\n",
    "        df['start_test']=start_test\n",
    "        df['split_id'] = split_id\n",
    "        split_data = ps.concat([split_data,df])\n",
    "\n",
    "        if(show_progress):\n",
    "            print(\"Split_id:\",split_id,\", Train period:\",start_train,\"-\" , start_test, \", test period\", start_test, \"-\", end_test)\n",
    "\n",
    "        # update dates:\n",
    "        end_test = end_test - relativedelta(days=stride)\n",
    "        start_test = end_test - relativedelta(days=test_period)\n",
    "        start_train = start_test - relativedelta(days=train_period)\n",
    "        split_id += 1\n",
    "    \n",
    "    return split_data, split_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e573f4-6c40-4cb0-a04c-2230ee9738ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data, split_id = split(tran_ps,TRAIN_PERIOD,7,30,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7446fe7-8a98-4d3b-95df-804cf4a96186",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08ff57c-714a-4013-abba-83622e50ed07",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87169e28-4c31-4acf-a0b0-d36649c0c5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e081df-ce79-4d81-a63a-834864bdebc2",
   "metadata": {},
   "source": [
    "# 3. 製作參數表 split_id => para_cross_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19af31c0-2956-45eb-a010-48ba1131c410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 製作參數表 paras_grid\n",
    "from itertools import product\n",
    "\n",
    "paras = list(\n",
    "    product(\n",
    "        [25,50,100,150,200],\n",
    "        [20,30,40,50],\n",
    "        [0.01]\n",
    "    )\n",
    ")\n",
    "paras_grid = ps.DataFrame(paras,columns= ['n_factors','n_epochs','reg_all'])\n",
    "paras_grid.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2e5a00-8133-411d-ad2f-d8555769b777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 製作 split_id 表\n",
    "split_id_ps = ps.DataFrame({'split_id': range(split_id)})\n",
    "split_id_ps.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7042a73-b886-4c89-bc11-fe0d58390b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將 paras_grid 與 split_id 做 cross join\n",
    "paras_grid['key'] = 1\n",
    "split_id_ps['key'] = 1\n",
    "\n",
    "para_cross_split = ps.merge(paras_grid, split_id_ps, on ='key').drop('key')\n",
    "del split_id_ps\n",
    "len(para_cross_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779b85df-084a-4fe9-92ab-593bd6a3735f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將 cross join 後的表新增遞增的 group_id 欄位，之後要用來做 pandas_udf 的 groupby\n",
    "para_cross_split['group_id'] = 0\n",
    "para_cross_split['group_id'] = np.arange(len(para_cross_split)).tolist()\n",
    "para_cross_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f927c99b-a1fd-4bca-b668-bb5239402a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# para_cross_split.to_parquet('/user/HM_parquet/SVD_model/para_cross_split.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e237f52-9033-4e3d-bca3-01f15621a19a",
   "metadata": {},
   "source": [
    "# 4. join 參數表和資料表 split_data, para_cross_split => join_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8cc8e2-ab37-4c93-9d2c-94ba36426f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "join_data = split_data.join(para_cross_split.set_index('split_id'), on='split_id')\n",
    "join_data.set_index('t_dat',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c0e02c-c4c8-4c35-b05d-2a3773404c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "join_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cace0df-e8bc-4310-ad49-4c8aefd26373",
   "metadata": {},
   "outputs": [],
   "source": [
    "39919883*20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a348c3-df01-4ea8-8c4f-1831f2a73da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "join_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1c57b7-32ce-472e-ba18-6a39d15d5800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join_data.to_parquet('/user/HM_parquet/SVD_model/join_data30.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba5c883-13c7-4909-b4d5-2f6b6c9906f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 刪除用不到的資料表\n",
    "del split_data, para_cross_split\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a347efaa-1f3b-4ca9-a904-6f13017347f1",
   "metadata": {},
   "source": [
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6567ccf7-b7a6-49f0-b995-d48fe917c7bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 讀取資料表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bab23a46-83fa-4cbd-9bd9-080f004a65da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "798397660"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.option('header','true').parquet('/user/HM_parquet/SVD_model/join_data30.parquet')\n",
    "# join_data.set_index('t_dat',inplace=True)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21d9cdb-574b-45a1-8386-9eb84b26a622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a46e2e-743b-4096-83d9-2ead5d1d7174",
   "metadata": {
    "tags": []
   },
   "source": [
    "# surpriseSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04a81400-b881-4dff-9e0f-7b5f542b3445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# from surprise import NormalPredictor\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise import SVDpp,SVD\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import average_precision as metrics\n",
    "\n",
    "class surpriseSVD():\n",
    "    def __init__(self):\n",
    "        self = self\n",
    "\n",
    "    def get_top_n(self, predictions, n=12):\n",
    "        \"\"\"Return the top-N recommendation for each user from a set of predictions.\n",
    "        Args:\n",
    "            predictions(list of Prediction objects): The list of predictions, as\n",
    "                returned by the test method of an algorithm.\n",
    "            n(int): The number of recommendation to output for each user. Default\n",
    "                is 10.\n",
    "        Returns:\n",
    "        A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "            [(raw item id, rating estimation), ...] of size n.\n",
    "        \"\"\"\n",
    "\n",
    "        # First map the predictions to each user.\n",
    "        top_n = defaultdict(list)\n",
    "        for uid, iid, true_r, est, _ in predictions:\n",
    "            top_n[uid].append((iid, est))\n",
    "\n",
    "        # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "        for uid, user_ratings in top_n.items():\n",
    "            user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "            top_n[uid] = user_ratings[:n]\n",
    "\n",
    "        return top_n\n",
    "\n",
    "    def get_set(self,df):\n",
    "        reader = Reader(rating_scale=(1, 500))\n",
    "        data_set = Dataset.load_from_df(df[['customer_id','article_id','rating']], reader)\n",
    "        return data_set\n",
    "\n",
    "    def get_rating_set(self,df):\n",
    "        rating = df[['customer_id','article_id','split_id']].groupby(['customer_id','article_id']).count().reset_index()\n",
    "        rating.columns = ['customer_id','article_id','rating']\n",
    "        rating_set = self.get_set(rating)\n",
    "        return rating_set\n",
    "\n",
    "\n",
    "    def train_SVD(self, train_data, test_data, paras={}):\n",
    "        \n",
    "        ## 讀取評分資料為surprise可以訓練的格式\n",
    "        trainset = self.get_rating_set(train_data)\n",
    "        testset = self.get_rating_set(test_data)\n",
    "\n",
    "        ## rmse 需要的資料\n",
    "        testset2 = [testset.df.loc[i].to_list() for i in range(len(testset.df))]\n",
    "\n",
    "        ## map@k testing 需要產的資料\n",
    "        test_data.loc[:,'rating']=0\n",
    "        test_processed = self.get_set(test_data)\n",
    "        NA, test2 = train_test_split(test_processed, test_size=1.0)\n",
    "\n",
    "        # ======= 消費者的實際購買清單 =======\n",
    "        test_data['article_id'] = test_data['article_id'].astype('str')\n",
    "        test_uni = test_data.drop_duplicates(subset=['customer_id', 'article_id'], keep='first')\n",
    "        buy_n = test_uni[['customer_id','article_id']].groupby('customer_id')['article_id'].apply(list).to_dict()\n",
    "\n",
    "        cust_actual_list = []\n",
    "        for uid, user_ratings in buy_n.items():\n",
    "            cust_pred_tuple = (uid, [iid for iid in user_ratings])\n",
    "            cust_actual_list.append(cust_pred_tuple)\n",
    "\n",
    "        # ======= 訓練 SVD 模型 =======\n",
    "        algo = SVD(random_state=42,**paras)\n",
    "\n",
    "        # 訓練模型\n",
    "        algo.fit(trainset.build_full_trainset())\n",
    "\n",
    "        ##### rmse #####\n",
    "        predictions = algo.test(testset2)\n",
    "        rmse = accuracy.rmse(predictions)\n",
    "\n",
    "        ##### map@k #####\n",
    "        predictions_map = algo.test(test2)\n",
    "        # est = [i.est for i in predictions_map] \n",
    "\n",
    "        ##  消費者的預測清單 \n",
    "        top_n = self.get_top_n(predictions=predictions_map, n=12)\n",
    "\n",
    "        cust_pred_list = []\n",
    "        for uid, user_ratings in top_n.items():\n",
    "            cust_pred_tuple = (uid, [str(iid) for (iid, _) in user_ratings])\n",
    "            cust_pred_list.append(cust_pred_tuple)\n",
    "\n",
    "        final_list = list(zip(cust_actual_list, cust_pred_list))\n",
    "\n",
    "        # map@k計算 \n",
    "        mapk_list = []\n",
    "        for i in range(len(final_list)):\n",
    "            map_k = metrics.mapk([final_list[i][0][1]],[final_list[i][1][1]],12)\n",
    "            mapk_list.append(map_k)\n",
    "\n",
    "        map_k = sum(mapk_list)/len(mapk_list)\n",
    "\n",
    "        return rmse, map_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c6af06-e9bb-4765-98eb-eef519527caf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# pandas_udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91f6e8a-fce2-4bf2-835e-e0e355aa914c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf = df.where(\"group_id == 43\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bace2342-f05b-443b-bd35-b37c2cc3e0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_split_hyperparameter_search(data):\n",
    "\n",
    "    paras = {\n",
    "        'n_factors':data.n_factors.values[0], \n",
    "        'n_epochs':data.n_epochs.values[0], \n",
    "        'reg_all':data.reg_all.values[0]\n",
    "    }\n",
    "    \n",
    "    test_index = data['t_dat'] > data['start_test'][0]\n",
    "    train_data = data.loc[~test_index]\n",
    "    test_data = data.loc[test_index]\n",
    "    \n",
    "    model = surpriseSVD()\n",
    "    rmse, map12 = model.train_SVD(train_data, test_data, paras)\n",
    "    \n",
    "    paras.update({\n",
    "        'stride': TRAIN_PERIOD,\n",
    "        'start_test' : data.start_test.values[0],\n",
    "        'rmse' : rmse,\n",
    "        'map12' : map12\n",
    "    })\n",
    "    \n",
    "    results = pd.DataFrame([paras])\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "edc5960d-e3cf-4b6e-b757-75cd9f34fdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    DateType, DoubleType, FloatType, IntegerType, StringType, StructField, StructType\n",
    ")\n",
    "# pandas_udf\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"stride\", IntegerType(), True),\n",
    "        StructField('start_test', DateType(),True),\n",
    "        StructField(\"n_factors\", IntegerType(), True),\n",
    "        StructField(\"n_epochs\", IntegerType(), True),\n",
    "        StructField('reg_all', FloatType(),True),\n",
    "        StructField('rmse', FloatType(),True),\n",
    "        StructField('map12', FloatType(),True),\n",
    "     ]\n",
    ")\n",
    "\n",
    "results = df.groupby('group_id').applyInPandas(time_split_hyperparameter_search, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1971d1e4-8493-40c0-8382-80e13c4bc89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 15:34:10,181 WARN scheduler.TaskSetManager: Lost task 94.0 in stage 12.0 (TID 306) (bdse42.example.com executor 4): FetchFailed(BlockManagerId(9, bdse91.example.com, 36595, None), shuffleId=5, mapIndex=11, mapId=198, reduceId=100, message=\n",
      "org.apache.spark.shuffle.FetchFailedException\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1165)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:903)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:84)\n",
      "\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator.isEmpty(Iterator.scala:387)\n",
      "\tat scala.collection.Iterator.isEmpty$(Iterator.scala:387)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.isEmpty(WholeStageCodegenExec.scala:757)\n",
      "\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasExec.$anonfun$doExecute$1(FlatMapGroupsInPandasExec.scala:81)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.ExecutorDeadException: The relative remote executor(Id: 9), which maintains the block data to fetch is dead.\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:136)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.start(RetryingBlockTransferor.java:133)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:146)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.sendRequest(ShuffleBlockFetcherIterator.scala:362)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.send$1(ShuffleBlockFetcherIterator.scala:1135)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchUpToMaxBytes(ShuffleBlockFetcherIterator.scala:1127)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:1000)\n",
      "\t... 37 more\n",
      "\n",
      ")\n",
      "2022-04-01 15:34:14,945 WARN scheduler.TaskSetManager: Lost task 92.0 in stage 12.0 (TID 299) (bdse108.example.com executor 11): FetchFailed(BlockManagerId(9, bdse91.example.com, 36595, None), shuffleId=5, mapIndex=2, mapId=188, reduceId=98, message=\n",
      "org.apache.spark.shuffle.FetchFailedException\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1165)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:903)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:84)\n",
      "\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator.isEmpty(Iterator.scala:387)\n",
      "\tat scala.collection.Iterator.isEmpty$(Iterator.scala:387)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.isEmpty(WholeStageCodegenExec.scala:757)\n",
      "\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasExec.$anonfun$doExecute$1(FlatMapGroupsInPandasExec.scala:81)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.ExecutorDeadException: The relative remote executor(Id: 9), which maintains the block data to fetch is dead.\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:136)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:184)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\t... 1 more\n",
      "\n",
      ")\n",
      "2022-04-01 15:34:32,588 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 9 for reason Container from a bad node: container_e32_1648793484157_0005_01_000011 on host: bdse91.example.com. Exit status: 137. Diagnostics: [2022-04-01 15:34:19.624]Container killed on request. Exit code is 137\n",
      "[2022-04-01 15:34:22.515]Container exited with a non-zero exit code 137. \n",
      "[2022-04-01 15:34:22.753]Killed by external signal\n",
      ".\n",
      "2022-04-01 15:34:32,590 ERROR cluster.YarnScheduler: Lost executor 9 on bdse91.example.com: Container from a bad node: container_e32_1648793484157_0005_01_000011 on host: bdse91.example.com. Exit status: 137. Diagnostics: [2022-04-01 15:34:19.624]Container killed on request. Exit code is 137\n",
      "[2022-04-01 15:34:22.515]Container exited with a non-zero exit code 137. \n",
      "[2022-04-01 15:34:22.753]Killed by external signal\n",
      ".\n",
      "2022-04-01 15:34:32,594 WARN scheduler.TaskSetManager: Lost task 44.0 in stage 12.0 (TID 283) (bdse91.example.com executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container from a bad node: container_e32_1648793484157_0005_01_000011 on host: bdse91.example.com. Exit status: 137. Diagnostics: [2022-04-01 15:34:19.624]Container killed on request. Exit code is 137\n",
      "[2022-04-01 15:34:22.515]Container exited with a non-zero exit code 137. \n",
      "[2022-04-01 15:34:22.753]Killed by external signal\n",
      ".\n",
      "2022-04-01 15:34:32,594 WARN scheduler.TaskSetManager: Lost task 71.0 in stage 12.0 (TID 294) (bdse91.example.com executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container from a bad node: container_e32_1648793484157_0005_01_000011 on host: bdse91.example.com. Exit status: 137. Diagnostics: [2022-04-01 15:34:19.624]Container killed on request. Exit code is 137\n",
      "[2022-04-01 15:34:22.515]Container exited with a non-zero exit code 137. \n",
      "[2022-04-01 15:34:22.753]Killed by external signal\n",
      ".\n",
      "2022-04-01 15:39:11,330 WARN scheduler.TaskSetManager: Lost task 7.0 in stage 12.1 (TID 325) (bdse91.example.com executor 12): FetchFailed(BlockManagerId(10, bdse90.example.com, 36467, None), shuffleId=5, mapIndex=19, mapId=202, reduceId=64, message=\n",
      "org.apache.spark.shuffle.FetchFailedException\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1165)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:903)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:84)\n",
      "\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator.isEmpty(Iterator.scala:387)\n",
      "\tat scala.collection.Iterator.isEmpty$(Iterator.scala:387)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.isEmpty(WholeStageCodegenExec.scala:757)\n",
      "\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasExec.$anonfun$doExecute$1(FlatMapGroupsInPandasExec.scala:81)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.io.IOException: Failed to connect to bdse90.example.com/172.22.33.90:36467\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:288)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:126)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:184)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\t... 1 more\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedNoRouteToHostException: No route to host: bdse90.example.com/172.22.33.90:36467\n",
      "Caused by: java.net.NoRouteToHostException: No route to host\n",
      "\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:330)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:707)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      ")\n",
      "2022-04-01 15:39:11,332 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 12.1 (TID 324) (bdse91.example.com executor 12): FetchFailed(BlockManagerId(10, bdse90.example.com, 36467, None), shuffleId=5, mapIndex=19, mapId=202, reduceId=5, message=\n",
      "org.apache.spark.shuffle.FetchFailedException\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1167)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:903)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:84)\n",
      "\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator.isEmpty(Iterator.scala:387)\n",
      "\tat scala.collection.Iterator.isEmpty$(Iterator.scala:387)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.isEmpty(WholeStageCodegenExec.scala:757)\n",
      "\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasExec.$anonfun$doExecute$1(FlatMapGroupsInPandasExec.scala:81)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.io.IOException: Connecting to bdse90.example.com/172.22.33.90:36467 failed in the last 4750 ms, fail this connection directly\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:126)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:184)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\t... 1 more\n",
      "\n",
      ")\n",
      "2022-04-01 15:39:54,451 WARN scheduler.TaskSetManager: Lost task 18.0 in stage 12.1 (TID 327) (bdse91.example.com executor 12): FetchFailed(BlockManagerId(7, bdse89.example.com, 36755, None), shuffleId=5, mapIndex=1, mapId=195, reduceId=103, message=\n",
      "org.apache.spark.shuffle.FetchFailedException\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1165)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:903)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:84)\n",
      "\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator.isEmpty(Iterator.scala:387)\n",
      "\tat scala.collection.Iterator.isEmpty$(Iterator.scala:387)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.isEmpty(WholeStageCodegenExec.scala:757)\n",
      "\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasExec.$anonfun$doExecute$1(FlatMapGroupsInPandasExec.scala:81)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.io.IOException: Failed to connect to bdse89.example.com/172.22.33.89:36755\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:288)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:126)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:184)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\t... 1 more\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedNoRouteToHostException: No route to host: bdse89.example.com/172.22.33.89:36755\n",
      "Caused by: java.net.NoRouteToHostException: No route to host\n",
      "\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:330)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:707)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      ")\n",
      "2022-04-01 15:54:18,775 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 10 for reason Container marked as failed: container_e32_1648793484157_0005_01_000012 on host: bdse90.example.com. Exit status: -100. Diagnostics: Container released on a *lost* node.\n",
      "2022-04-01 15:54:18,775 ERROR cluster.YarnScheduler: Lost executor 10 on bdse90.example.com: Container marked as failed: container_e32_1648793484157_0005_01_000012 on host: bdse90.example.com. Exit status: -100. Diagnostics: Container released on a *lost* node.\n",
      "2022-04-01 15:54:20,153 WARN server.TransportChannelHandler: Exception in connection from /172.22.33.90:48258\n",
      "java.io.IOException: Connection reset by peer\n",
      "\tat sun.nio.ch.FileDispatcherImpl.read0(Native Method)\n",
      "\tat sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)\n",
      "\tat sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)\n",
      "\tat sun.nio.ch.IOUtil.read(IOUtil.java:192)\n",
      "\tat sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)\n",
      "\tat io.netty.buffer.PooledByteBuf.setBytes(PooledByteBuf.java:253)\n",
      "\tat io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1132)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:350)\n",
      "\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:151)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "2022-04-01 16:00:07,144 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 7 for reason Container marked as failed: container_e32_1648793484157_0005_01_000009 on host: bdse89.example.com. Exit status: -100. Diagnostics: Container released on a *lost* node.\n",
      "2022-04-01 16:00:07,144 ERROR cluster.YarnScheduler: Lost executor 7 on bdse89.example.com: Container marked as failed: container_e32_1648793484157_0005_01_000009 on host: bdse89.example.com. Exit status: -100. Diagnostics: Container released on a *lost* node.\n",
      "2022-04-01 16:24:05,963 ERROR cluster.YarnScheduler: Lost executor 14 on bdse90.example.com: Container marked as failed: container_e32_1648793484157_0005_01_000016 on host: bdse90.example.com. Exit status: -100. Diagnostics: Container released on a *lost* node.\n",
      "2022-04-01 16:24:05,963 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 14 for reason Container marked as failed: container_e32_1648793484157_0005_01_000016 on host: bdse90.example.com. Exit status: -100. Diagnostics: Container released on a *lost* node.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "480"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:===================================================> (105 + -1) / 109]\r"
     ]
    }
   ],
   "source": [
    "results.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "972e1812-4e02-4cc5-9f1b-1725e24d0ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 19:43:22,068 WARN scheduler.TaskSetManager: Lost task 157.0 in stage 27.0 (TID 980) (bdse89.example.com executor 16): FetchFailed(BlockManagerId(4, bdse42.example.com, 44135, None), shuffleId=11, mapIndex=26, mapId=825, reduceId=174, message=\n",
      "org.apache.spark.shuffle.FetchFailedException\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1165)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:903)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:84)\n",
      "\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator.isEmpty(Iterator.scala:387)\n",
      "\tat scala.collection.Iterator.isEmpty$(Iterator.scala:387)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.isEmpty(WholeStageCodegenExec.scala:757)\n",
      "\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasExec.$anonfun$doExecute$1(FlatMapGroupsInPandasExec.scala:81)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.ExecutorDeadException: The relative remote executor(Id: 4), which maintains the block data to fetch is dead.\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:136)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:184)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\t... 1 more\n",
      "\n",
      ")\n",
      "2022-04-01 19:43:22,708 ERROR cluster.YarnScheduler: Lost executor 4 on bdse42.example.com: Container from a bad node: container_e32_1648793484157_0005_01_000006 on host: bdse42.example.com. Exit status: 137. Diagnostics: [2022-04-01 19:43:19.043]Container killed on request. Exit code is 137\n",
      "[2022-04-01 19:43:20.218]Container exited with a non-zero exit code 137. \n",
      "[2022-04-01 19:43:20.320]Killed by external signal\n",
      ".\n",
      "2022-04-01 19:43:22,708 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 4 for reason Container from a bad node: container_e32_1648793484157_0005_01_000006 on host: bdse42.example.com. Exit status: 137. Diagnostics: [2022-04-01 19:43:19.043]Container killed on request. Exit code is 137\n",
      "[2022-04-01 19:43:20.218]Container exited with a non-zero exit code 137. \n",
      "[2022-04-01 19:43:20.320]Killed by external signal\n",
      ".\n",
      "2022-04-01 19:43:22,709 WARN scheduler.TaskSetManager: Lost task 129.0 in stage 27.0 (TID 956) (bdse42.example.com executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container from a bad node: container_e32_1648793484157_0005_01_000006 on host: bdse42.example.com. Exit status: 137. Diagnostics: [2022-04-01 19:43:19.043]Container killed on request. Exit code is 137\n",
      "[2022-04-01 19:43:20.218]Container exited with a non-zero exit code 137. \n",
      "[2022-04-01 19:43:20.320]Killed by external signal\n",
      ".\n",
      "2022-04-01 19:43:22,709 WARN scheduler.TaskSetManager: Lost task 140.0 in stage 27.0 (TID 972) (bdse42.example.com executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container from a bad node: container_e32_1648793484157_0005_01_000006 on host: bdse42.example.com. Exit status: 137. Diagnostics: [2022-04-01 19:43:19.043]Container killed on request. Exit code is 137\n",
      "[2022-04-01 19:43:20.218]Container exited with a non-zero exit code 137. \n",
      "[2022-04-01 19:43:20.320]Killed by external signal\n",
      ".\n",
      "2022-04-01 19:43:31,733 WARN scheduler.TaskSetManager: Lost task 158.0 in stage 27.0 (TID 981) (bdse89.example.com executor 16): FetchFailed(BlockManagerId(4, bdse42.example.com, 44135, None), shuffleId=11, mapIndex=26, mapId=825, reduceId=175, message=\n",
      "org.apache.spark.shuffle.FetchFailedException\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1165)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:903)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:84)\n",
      "\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator.isEmpty(Iterator.scala:387)\n",
      "\tat scala.collection.Iterator.isEmpty$(Iterator.scala:387)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.isEmpty(WholeStageCodegenExec.scala:757)\n",
      "\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasExec.$anonfun$doExecute$1(FlatMapGroupsInPandasExec.scala:81)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.ExecutorDeadException: The relative remote executor(Id: 4), which maintains the block data to fetch is dead.\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:136)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.start(RetryingBlockTransferor.java:133)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:146)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.sendRequest(ShuffleBlockFetcherIterator.scala:362)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.send$1(ShuffleBlockFetcherIterator.scala:1135)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchUpToMaxBytes(ShuffleBlockFetcherIterator.scala:1127)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:1000)\n",
      "\t... 37 more\n",
      "\n",
      ")\n",
      "[Stage 12:============>(105 + -1) / 109][Stage 30:>                 (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+------------------+--------------------+\n",
      "|n_factors|n_epochs|reg_all|         avg(rmse)|          avg(map12)|\n",
      "+---------+--------+-------+------------------+--------------------+\n",
      "|       25|      20|   0.01| 21.12461559350292|5.767838883912191E-4|\n",
      "|       50|      20|   0.01|21.125548214962084|5.791609328298364E-4|\n",
      "|      100|      20|   0.01| 21.12736424803734| 5.80218322284054E-4|\n",
      "|       25|      30|   0.01| 21.12763550753395|5.803498206660151E-4|\n",
      "|       50|      30|   0.01|21.128597273180883|5.836315916288489E-4|\n",
      "+---------+--------+-------+------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:===================================================> (105 + -1) / 109]\r"
     ]
    }
   ],
   "source": [
    "results.groupBy(['n_factors','n_epochs','reg_all']).mean('rmse','map12').sort(\"avg(rmse)\").limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63d6107-0399-42f3-87c6-895c93ef4728",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 存檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88ac57e2-5202-4de7-b9af-2aa7e70d1b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 17:04:35,033 WARN scheduler.TaskSetManager: Lost task 36.0 in stage 19.0 (TID 541) (bdse106.example.com executor 2): TaskKilled (Stage cancelled)\n",
      "2022-04-01 17:04:35,033 WARN scheduler.TaskSetManager: Lost task 42.0 in stage 19.0 (TID 543) (bdse106.example.com executor 2): TaskKilled (Stage cancelled)\n",
      "2022-04-01 17:07:31,391 WARN scheduler.TaskSetManager: Lost task 13.0 in stage 19.0 (TID 503) (bdse91.example.com executor 12): TaskKilled (Stage cancelled)\n",
      "2022-04-01 17:45:15,407 WARN scheduler.TaskSetManager: Lost task 127.0 in stage 24.0 (TID 690) (bdse106.example.com executor 2): FetchFailed(BlockManagerId(13, bdse89.example.com, 41377, None), shuffleId=10, mapIndex=22, mapId=563, reduceId=137, message=\n",
      "org.apache.spark.shuffle.FetchFailedException\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1165)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:903)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:84)\n",
      "\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator.isEmpty(Iterator.scala:387)\n",
      "\tat scala.collection.Iterator.isEmpty$(Iterator.scala:387)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.isEmpty(WholeStageCodegenExec.scala:757)\n",
      "\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasExec.$anonfun$doExecute$1(FlatMapGroupsInPandasExec.scala:81)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.ExecutorDeadException: The relative remote executor(Id: 13), which maintains the block data to fetch is dead.\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:136)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.start(RetryingBlockTransferor.java:133)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:146)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.sendRequest(ShuffleBlockFetcherIterator.scala:362)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.send$1(ShuffleBlockFetcherIterator.scala:1135)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchUpToMaxBytes(ShuffleBlockFetcherIterator.scala:1127)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:1000)\n",
      "\t... 32 more\n",
      "\n",
      ")\n",
      "2022-04-01 17:45:25,889 WARN scheduler.TaskSetManager: Lost task 151.0 in stage 24.0 (TID 694) (bdse106.example.com executor 2): FetchFailed(BlockManagerId(13, bdse89.example.com, 41377, None), shuffleId=10, mapIndex=22, mapId=563, reduceId=168, message=\n",
      "org.apache.spark.shuffle.FetchFailedException\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1165)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:903)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:84)\n",
      "\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator.isEmpty(Iterator.scala:387)\n",
      "\tat scala.collection.Iterator.isEmpty$(Iterator.scala:387)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.isEmpty(WholeStageCodegenExec.scala:757)\n",
      "\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasExec.$anonfun$doExecute$1(FlatMapGroupsInPandasExec.scala:81)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.ExecutorDeadException: The relative remote executor(Id: 13), which maintains the block data to fetch is dead.\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:136)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.start(RetryingBlockTransferor.java:133)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:146)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.sendRequest(ShuffleBlockFetcherIterator.scala:362)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.send$1(ShuffleBlockFetcherIterator.scala:1135)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchUpToMaxBytes(ShuffleBlockFetcherIterator.scala:1127)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:1000)\n",
      "\t... 32 more\n",
      "\n",
      ")\n",
      "2022-04-01 17:46:07,155 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 13 for reason Container from a bad node: container_e32_1648793484157_0005_01_000015 on host: bdse89.example.com. Exit status: 137. Diagnostics: [2022-04-01 17:45:44.659]Container killed on request. Exit code is 137\n",
      "[2022-04-01 17:45:51.899]Container exited with a non-zero exit code 137. \n",
      "[2022-04-01 17:45:53.854]Killed by external signal\n",
      ".\n",
      "2022-04-01 17:46:07,155 ERROR cluster.YarnScheduler: Lost executor 13 on bdse89.example.com: Container from a bad node: container_e32_1648793484157_0005_01_000015 on host: bdse89.example.com. Exit status: 137. Diagnostics: [2022-04-01 17:45:44.659]Container killed on request. Exit code is 137\n",
      "[2022-04-01 17:45:51.899]Container exited with a non-zero exit code 137. \n",
      "[2022-04-01 17:45:53.854]Killed by external signal\n",
      ".\n",
      "2022-04-01 17:46:07,156 WARN scheduler.TaskSetManager: Lost task 101.0 in stage 24.0 (TID 675) (bdse89.example.com executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container from a bad node: container_e32_1648793484157_0005_01_000015 on host: bdse89.example.com. Exit status: 137. Diagnostics: [2022-04-01 17:45:44.659]Container killed on request. Exit code is 137\n",
      "[2022-04-01 17:45:51.899]Container exited with a non-zero exit code 137. \n",
      "[2022-04-01 17:45:53.854]Killed by external signal\n",
      ".\n",
      "2022-04-01 17:46:07,156 WARN scheduler.TaskSetManager: Lost task 69.0 in stage 24.0 (TID 642) (bdse89.example.com executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container from a bad node: container_e32_1648793484157_0005_01_000015 on host: bdse89.example.com. Exit status: 137. Diagnostics: [2022-04-01 17:45:44.659]Container killed on request. Exit code is 137\n",
      "[2022-04-01 17:45:51.899]Container exited with a non-zero exit code 137. \n",
      "[2022-04-01 17:45:53.854]Killed by external signal\n",
      ".\n",
      "2022-04-01 17:53:16,153 WARN scheduler.TaskSetManager: Lost task 9.0 in stage 24.1 (TID 707) (bdse90.example.com executor 15): TaskCommitDenied (Driver denied task commit) for job: 24, partition: 105, attemptNumber: 0\n",
      "2022-04-01 17:53:39,241 WARN scheduler.TaskSetManager: Lost task 6.0 in stage 24.1 (TID 703) (bdse109.example.com executor 8): TaskCommitDenied (Driver denied task commit) for job: 24, partition: 99, attemptNumber: 0\n",
      "2022-04-01 17:54:31,222 WARN scheduler.TaskSetManager: Lost task 7.0 in stage 24.1 (TID 702) (bdse89.example.com executor 16): TaskCommitDenied (Driver denied task commit) for job: 24, partition: 100, attemptNumber: 0\n",
      "2022-04-01 17:55:15,187 WARN scheduler.TaskSetManager: Lost task 2.0 in stage 24.1 (TID 699) (bdse106.example.com executor 2): TaskCommitDenied (Driver denied task commit) for job: 24, partition: 83, attemptNumber: 0\n",
      "2022-04-01 17:57:19,661 ERROR cluster.YarnScheduler: Lost executor 3 on bdse74.example.com: Container from a bad node: container_e32_1648793484157_0005_01_000005 on host: bdse74.example.com. Exit status: 137. Diagnostics: [2022-04-01 17:57:17.282]Container killed on request. Exit code is 137\n",
      "[2022-04-01 17:57:17.551]Container exited with a non-zero exit code 137. \n",
      "[2022-04-01 17:57:17.586]Killed by external signal\n",
      ".\n",
      "2022-04-01 17:57:19,661 WARN scheduler.TaskSetManager: Lost task 4.0 in stage 24.1 (TID 705) (bdse74.example.com executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container from a bad node: container_e32_1648793484157_0005_01_000005 on host: bdse74.example.com. Exit status: 137. Diagnostics: [2022-04-01 17:57:17.282]Container killed on request. Exit code is 137\n",
      "[2022-04-01 17:57:17.551]Container exited with a non-zero exit code 137. \n",
      "[2022-04-01 17:57:17.586]Killed by external signal\n",
      ".\n",
      "2022-04-01 17:57:19,662 WARN scheduler.TaskSetManager: Lost task 11.0 in stage 24.1 (TID 708) (bdse74.example.com executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container from a bad node: container_e32_1648793484157_0005_01_000005 on host: bdse74.example.com. Exit status: 137. Diagnostics: [2022-04-01 17:57:17.282]Container killed on request. Exit code is 137\n",
      "[2022-04-01 17:57:17.551]Container exited with a non-zero exit code 137. \n",
      "[2022-04-01 17:57:17.586]Killed by external signal\n",
      ".\n",
      "2022-04-01 17:57:19,662 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 3 for reason Container from a bad node: container_e32_1648793484157_0005_01_000005 on host: bdse74.example.com. Exit status: 137. Diagnostics: [2022-04-01 17:57:17.282]Container killed on request. Exit code is 137\n",
      "[2022-04-01 17:57:17.551]Container exited with a non-zero exit code 137. \n",
      "[2022-04-01 17:57:17.586]Killed by external signal\n",
      ".\n",
      "2022-04-01 17:57:31,727 WARN scheduler.TaskSetManager: Lost task 17.0 in stage 24.1 (TID 723) (bdse74.example.com executor 17): FetchFailed(null, shuffleId=10, mapIndex=-1, mapId=-1, reduceId=125, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 10 partition 125\n",
      "\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1619)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10(MapOutputTracker.scala:1566)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10$adapted(MapOutputTracker.scala:1565)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1565)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1230)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1192)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n",
      "\tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:208)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      ")\n",
      "2022-04-01 17:57:31,731 WARN scheduler.TaskSetManager: Lost task 22.0 in stage 24.1 (TID 724) (bdse74.example.com executor 17): FetchFailed(null, shuffleId=10, mapIndex=-1, mapId=-1, reduceId=130, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 10 partition 130\n",
      "\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1619)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10(MapOutputTracker.scala:1566)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10$adapted(MapOutputTracker.scala:1565)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1565)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1230)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1192)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n",
      "\tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:208)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      ")\n",
      "2022-04-01 17:57:31,756 WARN scheduler.TaskSetManager: Lost task 33.0 in stage 24.1 (TID 725) (bdse74.example.com executor 17): FetchFailed(null, shuffleId=10, mapIndex=-1, mapId=-1, reduceId=143, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 10 partition 143\n",
      "\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1619)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10(MapOutputTracker.scala:1566)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$10$adapted(MapOutputTracker.scala:1565)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1565)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1230)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1192)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n",
      "\tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:208)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      ")\n",
      "2022-04-01 17:57:45,164 WARN scheduler.TaskSetManager: Lost task 45.0 in stage 24.1 (TID 711) (bdse91.example.com executor 12): FetchFailed(BlockManagerId(3, bdse74.example.com, 35803, None), shuffleId=10, mapIndex=25, mapId=574, reduceId=161, message=\n",
      "org.apache.spark.shuffle.FetchFailedException\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1165)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:903)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:84)\n",
      "\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator.isEmpty(Iterator.scala:387)\n",
      "\tat scala.collection.Iterator.isEmpty$(Iterator.scala:387)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.isEmpty(WholeStageCodegenExec.scala:757)\n",
      "\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasExec.$anonfun$doExecute$1(FlatMapGroupsInPandasExec.scala:81)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.ExecutorDeadException: The relative remote executor(Id: 3), which maintains the block data to fetch is dead.\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:136)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.start(RetryingBlockTransferor.java:133)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:146)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.sendRequest(ShuffleBlockFetcherIterator.scala:362)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.send$1(ShuffleBlockFetcherIterator.scala:1135)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchUpToMaxBytes(ShuffleBlockFetcherIterator.scala:1127)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:1000)\n",
      "\t... 32 more\n",
      "\n",
      ")\n",
      "2022-04-01 17:58:15,945 WARN scheduler.TaskSetManager: Lost task 13.0 in stage 24.1 (TID 698) (bdse108.example.com executor 11): TaskCommitDenied (Driver denied task commit) for job: 24, partition: 112, attemptNumber: 0\n",
      "2022-04-01 18:02:35,082 WARN scheduler.TaskSetManager: Lost task 3.0 in stage 24.1 (TID 700) (bdse93.example.com executor 5): TaskCommitDenied (Driver denied task commit) for job: 24, partition: 87, attemptNumber: 0\n",
      "2022-04-01 18:03:03,418 WARN scheduler.TaskSetManager: Lost task 10.0 in stage 24.1 (TID 706) (bdse93.example.com executor 5): TaskCommitDenied (Driver denied task commit) for job: 24, partition: 107, attemptNumber: 0\n",
      "2022-04-01 18:04:12,755 WARN scheduler.TaskSetManager: Lost task 21.0 in stage 24.1 (TID 715) (bdse90.example.com executor 15): TaskCommitDenied (Driver denied task commit) for job: 24, partition: 120, attemptNumber: 0\n",
      "2022-04-01 18:04:52,821 WARN scheduler.TaskSetManager: Lost task 23.0 in stage 24.1 (TID 721) (bdse75.example.com executor 1): TaskCommitDenied (Driver denied task commit) for job: 24, partition: 122, attemptNumber: 0\n",
      "2022-04-01 18:05:40,336 WARN scheduler.TaskSetManager: Lost task 5.0 in stage 24.1 (TID 704) (bdse106.example.com executor 2): TaskCommitDenied (Driver denied task commit) for job: 24, partition: 96, attemptNumber: 0\n",
      "2022-04-01 18:07:51,337 WARN scheduler.TaskSetManager: Lost task 11.0 in stage 24.2 (TID 738) (bdse74.example.com executor 17): TaskCommitDenied (Driver denied task commit) for job: 24, partition: 134, attemptNumber: 0\n",
      "2022-04-01 18:12:24,758 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 24.2 (TID 734) (bdse106.example.com executor 2): TaskCommitDenied (Driver denied task commit) for job: 24, partition: 101, attemptNumber: 0\n",
      "2022-04-01 18:19:35,297 WARN scheduler.TaskSetManager: Lost task 20.0 in stage 24.1 (TID 710) (bdse91.example.com executor 12): TaskCommitDenied (Driver denied task commit) for job: 24, partition: 119, attemptNumber: 0\n",
      "2022-04-01 18:23:07,757 ERROR cluster.YarnScheduler: Lost executor 6 on bdse92.example.com: Container from a bad node: container_e32_1648793484157_0005_01_000008 on host: bdse92.example.com. Exit status: 137. Diagnostics: [2022-04-01 18:23:02.252]Container killed on request. Exit code is 137\n",
      "[2022-04-01 18:23:04.625]Container exited with a non-zero exit code 137. \n",
      "[2022-04-01 18:23:04.746]Killed by external signal\n",
      ".\n",
      "2022-04-01 18:23:07,758 WARN scheduler.TaskSetManager: Lost task 25.0 in stage 24.2 (TID 761) (bdse92.example.com executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container from a bad node: container_e32_1648793484157_0005_01_000008 on host: bdse92.example.com. Exit status: 137. Diagnostics: [2022-04-01 18:23:02.252]Container killed on request. Exit code is 137\n",
      "[2022-04-01 18:23:04.625]Container exited with a non-zero exit code 137. \n",
      "[2022-04-01 18:23:04.746]Killed by external signal\n",
      ".\n",
      "2022-04-01 18:23:07,758 WARN scheduler.TaskSetManager: Lost task 6.0 in stage 24.2 (TID 736) (bdse92.example.com executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container from a bad node: container_e32_1648793484157_0005_01_000008 on host: bdse92.example.com. Exit status: 137. Diagnostics: [2022-04-01 18:23:02.252]Container killed on request. Exit code is 137\n",
      "[2022-04-01 18:23:04.625]Container exited with a non-zero exit code 137. \n",
      "[2022-04-01 18:23:04.746]Killed by external signal\n",
      ".\n",
      "2022-04-01 18:23:07,761 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 6 for reason Container from a bad node: container_e32_1648793484157_0005_01_000008 on host: bdse92.example.com. Exit status: 137. Diagnostics: [2022-04-01 18:23:02.252]Container killed on request. Exit code is 137\n",
      "[2022-04-01 18:23:04.625]Container exited with a non-zero exit code 137. \n",
      "[2022-04-01 18:23:04.746]Killed by external signal\n",
      ".\n",
      "2022-04-01 18:23:12,016 WARN scheduler.TaskSetManager: Lost task 6.1 in stage 24.2 (TID 788) (bdse93.example.com executor 5): FetchFailed(BlockManagerId(6, bdse92.example.com, 39663, None), shuffleId=10, mapIndex=10, mapId=552, reduceId=140, message=\n",
      "org.apache.spark.shuffle.FetchFailedException\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1165)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:903)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:84)\n",
      "\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator.isEmpty(Iterator.scala:387)\n",
      "\tat scala.collection.Iterator.isEmpty$(Iterator.scala:387)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.isEmpty(WholeStageCodegenExec.scala:757)\n",
      "\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasExec.$anonfun$doExecute$1(FlatMapGroupsInPandasExec.scala:81)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.ExecutorDeadException: The relative remote executor(Id: 6), which maintains the block data to fetch is dead.\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:136)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.start(RetryingBlockTransferor.java:133)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:146)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.sendRequest(ShuffleBlockFetcherIterator.scala:362)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.send$1(ShuffleBlockFetcherIterator.scala:1135)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchUpToMaxBytes(ShuffleBlockFetcherIterator.scala:1127)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:701)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:191)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:89)\n",
      "\tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:240)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\t... 14 more\n",
      "\n",
      ")\n",
      "2022-04-01 18:23:12,101 WARN scheduler.TaskSetManager: Lost task 25.1 in stage 24.2 (TID 789) (bdse90.example.com executor 15): FetchFailed(BlockManagerId(6, bdse92.example.com, 39663, None), shuffleId=10, mapIndex=10, mapId=552, reduceId=166, message=\n",
      "org.apache.spark.shuffle.FetchFailedException\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1165)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:903)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:84)\n",
      "\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator.isEmpty(Iterator.scala:387)\n",
      "\tat scala.collection.Iterator.isEmpty$(Iterator.scala:387)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.isEmpty(WholeStageCodegenExec.scala:757)\n",
      "\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasExec.$anonfun$doExecute$1(FlatMapGroupsInPandasExec.scala:81)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.ExecutorDeadException: The relative remote executor(Id: 6), which maintains the block data to fetch is dead.\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:136)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.start(RetryingBlockTransferor.java:133)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:146)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.sendRequest(ShuffleBlockFetcherIterator.scala:362)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.send$1(ShuffleBlockFetcherIterator.scala:1135)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchUpToMaxBytes(ShuffleBlockFetcherIterator.scala:1127)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:1000)\n",
      "\t... 32 more\n",
      "\n",
      ")\n",
      "2022-04-01 18:34:09,066 WARN scheduler.TaskSetManager: Lost task 4.0 in stage 24.3 (TID 796) (bdse75.example.com executor 1): TaskCommitDenied (Driver denied task commit) for job: 24, partition: 163, attemptNumber: 0\n",
      "2022-04-01 18:37:00,617 WARN scheduler.TaskSetManager: Lost task 14.0 in stage 24.2 (TID 776) (bdse108.example.com executor 11): TaskCommitDenied (Driver denied task commit) for job: 24, partition: 137, attemptNumber: 0\n",
      "2022-04-01 18:38:29,074 WARN scheduler.TaskSetManager: Lost task 5.0 in stage 24.3 (TID 792) (bdse42.example.com executor 4): TaskCommitDenied (Driver denied task commit) for job: 24, partition: 174, attemptNumber: 0\n",
      "2022-04-01 18:39:39,133 WARN scheduler.TaskSetManager: Lost task 7.0 in stage 24.3 (TID 793) (bdse90.example.com executor 15): TaskCommitDenied (Driver denied task commit) for job: 24, partition: 179, attemptNumber: 0\n",
      "2022-04-01 18:40:04,909 WARN scheduler.TaskSetManager: Lost task 3.0 in stage 24.3 (TID 794) (bdse75.example.com executor 1): TaskKilled (Stage finished)\n",
      "2022-04-01 18:40:13,221 WARN scheduler.TaskSetManager: Lost task 50.0 in stage 24.1 (TID 714) (bdse109.example.com executor 8): TaskKilled (Stage finished)\n",
      "2022-04-01 18:40:14,335 WARN scheduler.TaskSetManager: Lost task 18.0 in stage 24.1 (TID 709) (bdse109.example.com executor 8): TaskKilled (Stage finished)\n",
      "2022-04-01 18:40:43,165 WARN scheduler.TaskSetManager: Lost task 6.0 in stage 24.3 (TID 798) (bdse91.example.com executor 12): TaskKilled (Stage finished)\n",
      "[Stage 12:===================================================> (105 + -1) / 109]\r"
     ]
    }
   ],
   "source": [
    "results.write.parquet(f'/user/HM_parquet/SVD_model/params/para{TRAIN_PERIOD}.parquet',mode='overwrite',partitionBy='rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e44e71-2904-4dbd-9ccc-d2090835a62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.write.parquet('file:///home/hadoop/git_workspace/BDSE23_HM_recommendation/spark/params/para_30.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798f1c17-50a2-409d-8974-2c965feb718c",
   "metadata": {},
   "outputs": [],
   "source": [
    "para30_ps['stride']=30\n",
    "para30_ps.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a38231-5a0e-4cbe-975a-1b71cd8abf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "para30_ps.to_parquet('/user/HM_parquet/SVD_model/params/para30.parquet',mode = 'overwrite',partition_cols='start_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad980274-0a6c-4779-b2bb-7c41927051e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "para30 = ps.read_parquet('/user/HM_parquet/SVD_model/para_30.parquet',columns=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5348c4-3990-44eb-84b9-2eed404cc61b",
   "metadata": {},
   "source": [
    "# (DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b996c3-1b5e-4f11-8776-c5a589f457b2",
   "metadata": {},
   "source": [
    "## 1. 讀取檔案 /DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1431f87d-52d0-4c07-9609-ac6e977ea0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 讀取檔案\n",
    "# customers = spark.read.option('header','true').parquet('/user/HM_parquet/customers.parquet')\n",
    "# articles = spark.read.option('header','true').parquet('/user/HM_parquet/articles.parquet')\n",
    "# transactions = spark.read.option('header','true').parquet('/user/HM_parquet/transactions_train.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a841413-2d9a-4f01-ba53-5d301b39082d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transactions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1012b563-2eb7-40c8-8d2a-39a119f9e288",
   "metadata": {},
   "source": [
    "## 2. 將customer_id(字串)轉為customer_index(整數) /DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634903ba-7100-4a88-9f24-7becc2e1c40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 將customers的customer_id轉為數字(buffer要增加到512m)\n",
    "# toIndex = StringIndexer(inputCol=\"customer_id\", outputCol=\"customer_index\").fit(customers)\n",
    "# customers = toIndex.transform(customers)\n",
    "# # customers.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2875dbbe-f263-4550-bca3-79c887a4901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 將transactions的customer_id轉為數字\n",
    "# transactions = toIndex.transform(transactions)\n",
    "# # transactions.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf64ded3-e050-46e5-a4c2-cebf60cb422d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transactions.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
