{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ee3d7fd-c0c3-47bf-820b-cb2b94e70908",
   "metadata": {},
   "source": [
    "# 0. 設定 Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "458744cd-e2db-463e-a419-0ad9e6145e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://bdse111.example.com:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>maggie</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7efc1cd7e700>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3706663a-2717-490c-8787-adaf1af62898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark.pandas as ps\n",
    "import numpy as np\n",
    "import gc\n",
    "import pyspark.sql.functions as sf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import array_max\n",
    "from pyspark.ml.feature import StringIndexer, IndexToString\n",
    "from pyspark.pandas.config import set_option, reset_option\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e94b47e-b307-4b85-9c1e-85056953d516",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", True)\n",
    "ps.set_option(\"compute.default_index_type\", \"distributed\")\n",
    "set_option(\"compute.ops_on_diff_frames\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b71f5416-468d-48ff-9a59-c12a5ca92041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'512m'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.kryoserializer.buffer.max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1211e0ab-6b6c-4e2f-a7f0-48dde3c45262",
   "metadata": {},
   "source": [
    "# 1. 讀取檔案 => tran_ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "818ed44a-369c-4a67-a68a-aee7f5bf83ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取檔案\n",
    "# customers = ps.read_parquet('/user/HM_parquet/customers.parquet')\n",
    "# articles = ps.read_parquet('/user/HM_parquet/articles.parquet')\n",
    "tran_ps = ps.read_parquet('/user/HM_parquet/transactions_train.parquet').drop(['price', 'sales_channel_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8881aeab-fa63-4d2f-8f4b-fc371e3ef2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>start_test</th>\n",
       "      <th>split_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_dat</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-09-20</th>\n",
       "      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n",
       "      <td>663713001</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-20</th>\n",
       "      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n",
       "      <td>541518023</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-20</th>\n",
       "      <td>00007d2de826758b65a93dd24ce629ed66842531df6699...</td>\n",
       "      <td>505221004</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-20</th>\n",
       "      <td>00007d2de826758b65a93dd24ce629ed66842531df6699...</td>\n",
       "      <td>685687003</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-20</th>\n",
       "      <td>00007d2de826758b65a93dd24ce629ed66842531df6699...</td>\n",
       "      <td>685687004</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 customer_id  article_id start_test split_id\n",
       "t_dat                                                                                                       \n",
       "2018-09-20  000058a12d5b43e67d225668fa1f8d618c13dc232df0cad8ffe7ad4a1091e318   663713001                    \n",
       "2018-09-20  000058a12d5b43e67d225668fa1f8d618c13dc232df0cad8ffe7ad4a1091e318   541518023                    \n",
       "2018-09-20  00007d2de826758b65a93dd24ce629ed66842531df6699338c5570910a014cc2   505221004                    \n",
       "2018-09-20  00007d2de826758b65a93dd24ce629ed66842531df6699338c5570910a014cc2   685687003                    \n",
       "2018-09-20  00007d2de826758b65a93dd24ce629ed66842531df6699338c5570910a014cc2   685687004                    "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tran_ps.set_index('t_dat',inplace=True)\n",
    "tran_ps['start_test'] = ''\n",
    "tran_ps['split_id'] = ''\n",
    "tran_ps.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b66614e-1b55-43e3-a632-42d05d0be6ef",
   "metadata": {},
   "source": [
    "# 2. 作時間分割 tran_ps => split_data, split_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9079e109-bcd4-4284-a78c-e8f4b6fbcc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(data,train_period=30, test_period=7, stride=30,show_progress=False):\n",
    "    \n",
    "    split_data = ps.DataFrame(columns = ['t_dat','customer_id', 'article_id', 'split_id', 'start_test']).set_index('t_dat',inplace=True)\n",
    "\n",
    "    end_test = data.index.max()\n",
    "    start_test = end_test - relativedelta(days=test_period)\n",
    "    start_train = start_test - relativedelta(days=train_period)\n",
    "    split_id=0\n",
    "\n",
    "    while start_train >= data.index.min():\n",
    "\n",
    "        df = data.loc[start_train:end_test]\n",
    "        df['start_test']=start_test\n",
    "        df['split_id'] = split_id\n",
    "        split_data = ps.concat([split_data,df])\n",
    "\n",
    "        if(show_progress):\n",
    "            print(\"Split_id:\",split_id,\", Train period:\",start_train,\"-\" , start_test, \", test period\", start_test, \"-\", end_test)\n",
    "\n",
    "        # update dates:\n",
    "        end_test = end_test - relativedelta(days=stride)\n",
    "        start_test = end_test - relativedelta(days=test_period)\n",
    "        start_train = start_test - relativedelta(days=train_period)\n",
    "        split_id += 1\n",
    "    \n",
    "    return split_data, split_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28e573f4-6c40-4cb0-a04c-2230ee9738ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split_id: 0 , Train period: 2020-08-16 - 2020-09-15 , test period 2020-09-15 - 2020-09-22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split_id: 1 , Train period: 2020-07-17 - 2020-08-16 , test period 2020-08-16 - 2020-08-23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split_id: 2 , Train period: 2020-06-17 - 2020-07-17 , test period 2020-07-17 - 2020-07-24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split_id: 3 , Train period: 2020-05-18 - 2020-06-17 , test period 2020-06-17 - 2020-06-24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split_id: 4 , Train period: 2020-04-18 - 2020-05-18 , test period 2020-05-18 - 2020-05-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split_id: 5 , Train period: 2020-03-19 - 2020-04-18 , test period 2020-04-18 - 2020-04-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split_id: 6 , Train period: 2020-02-18 - 2020-03-19 , test period 2020-03-19 - 2020-03-26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split_id: 7 , Train period: 2020-01-19 - 2020-02-18 , test period 2020-02-18 - 2020-02-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split_id: 8 , Train period: 2019-12-20 - 2020-01-19 , test period 2020-01-19 - 2020-01-26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split_id: 9 , Train period: 2019-11-20 - 2019-12-20 , test period 2019-12-20 - 2019-12-27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split_id: 10 , Train period: 2019-10-21 - 2019-11-20 , test period 2019-11-20 - 2019-11-27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split_id: 11 , Train period: 2019-09-21 - 2019-10-21 , test period 2019-10-21 - 2019-10-28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split_id: 12 , Train period: 2019-08-22 - 2019-09-21 , test period 2019-09-21 - 2019-09-28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split_id: 13 , Train period: 2019-07-23 - 2019-08-22 , test period 2019-08-22 - 2019-08-29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split_id: 14 , Train period: 2019-06-23 - 2019-07-23 , test period 2019-07-23 - 2019-07-30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split_id: 15 , Train period: 2019-05-24 - 2019-06-23 , test period 2019-06-23 - 2019-06-30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split_id: 16 , Train period: 2019-04-24 - 2019-05-24 , test period 2019-05-24 - 2019-05-31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split_id: 17 , Train period: 2019-03-25 - 2019-04-24 , test period 2019-04-24 - 2019-05-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split_id: 18 , Train period: 2019-02-23 - 2019-03-25 , test period 2019-03-25 - 2019-04-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split_id: 19 , Train period: 2019-01-24 - 2019-02-23 , test period 2019-02-23 - 2019-03-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split_id: 20 , Train period: 2018-12-25 - 2019-01-24 , test period 2019-01-24 - 2019-01-31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split_id: 21 , Train period: 2018-11-25 - 2018-12-25 , test period 2018-12-25 - 2019-01-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split_id: 22 , Train period: 2018-10-26 - 2018-11-25 , test period 2018-11-25 - 2018-12-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split_id: 23 , Train period: 2018-09-26 - 2018-10-26 , test period 2018-10-26 - 2018-11-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "split_data, split_id = split(tran_ps,30,7,30,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7446fe7-8a98-4d3b-95df-804cf4a96186",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a08ff57c-714a-4013-abba-83622e50ed07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "start_test     39919883\n",
       "t_dat          39919883\n",
       "customer_id    39919883\n",
       "article_id     39919883\n",
       "split_id       39919883\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e081df-ce79-4d81-a63a-834864bdebc2",
   "metadata": {},
   "source": [
    "# 3. 製作參數表 split_id => para_cross_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19af31c0-2956-45eb-a010-48ba1131c410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 製作參數表 paras_grid\n",
    "from itertools import product\n",
    "\n",
    "paras = list(\n",
    "    product(\n",
    "        [25,50,100,150,200],\n",
    "        [20,30,40,50],\n",
    "        [0.01]\n",
    "    )\n",
    ")\n",
    "paras_grid = ps.DataFrame(paras,columns= ['n_factors','n_epochs','reg_all'])\n",
    "paras_grid.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2e5a00-8133-411d-ad2f-d8555769b777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 製作 split_id 表\n",
    "split_id_ps = ps.DataFrame({'split_id': range(split_id)})\n",
    "split_id_ps.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7042a73-b886-4c89-bc11-fe0d58390b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將 paras_grid 與 split_id 做 cross join\n",
    "paras_grid['key'] = 1\n",
    "split_id_ps['key'] = 1\n",
    "\n",
    "para_cross_split = ps.merge(paras_grid, split_id_ps, on ='key').drop('key')\n",
    "del split_id_ps\n",
    "len(para_cross_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779b85df-084a-4fe9-92ab-593bd6a3735f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將 cross join 後的表新增遞增的 group_id 欄位，之後要用來做 pandas_udf 的 groupby\n",
    "para_cross_split['group_id'] = 0\n",
    "para_cross_split['group_id'] = np.arange(len(para_cross_split)).tolist()\n",
    "para_cross_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f927c99b-a1fd-4bca-b668-bb5239402a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# para_cross_split.to_parquet('/user/HM_parquet/SVD_model/para_cross_split.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e237f52-9033-4e3d-bca3-01f15621a19a",
   "metadata": {},
   "source": [
    "# 4. join 參數表和資料表 split_data, para_cross_split => join_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8cc8e2-ab37-4c93-9d2c-94ba36426f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "join_data = split_data.join(para_cross_split.set_index('split_id'), on='split_id')\n",
    "join_data.set_index('t_dat',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c0e02c-c4c8-4c35-b05d-2a3773404c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "join_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cace0df-e8bc-4310-ad49-4c8aefd26373",
   "metadata": {},
   "outputs": [],
   "source": [
    "39919883*20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a348c3-df01-4ea8-8c4f-1831f2a73da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "join_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1c57b7-32ce-472e-ba18-6a39d15d5800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join_data.to_parquet('/user/HM_parquet/SVD_model/join_data30.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba5c883-13c7-4909-b4d5-2f6b6c9906f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 刪除用不到的資料表\n",
    "del split_data, para_cross_split\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a347efaa-1f3b-4ca9-a904-6f13017347f1",
   "metadata": {},
   "source": [
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6567ccf7-b7a6-49f0-b995-d48fe917c7bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 讀取資料表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bab23a46-83fa-4cbd-9bd9-080f004a65da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "n_factors      798397660\n",
       "group_id       798397660\n",
       "start_test     798397660\n",
       "n_epochs       798397660\n",
       "customer_id    798397660\n",
       "reg_all        798397660\n",
       "article_id     798397660\n",
       "split_id       798397660\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join_data = ps.read_parquet('/user/HM_parquet/SVD_model/join_data30.parquet')\n",
    "join_data.set_index('t_dat',inplace=True)\n",
    "join_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f21d9cdb-574b-45a1-8386-9eb84b26a622",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>start_test</th>\n",
       "      <th>n_factors</th>\n",
       "      <th>n_epochs</th>\n",
       "      <th>reg_all</th>\n",
       "      <th>group_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_dat</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-06-23</th>\n",
       "      <td>14</td>\n",
       "      <td>00007d2de826758b65a93dd24ce629ed66842531df6699...</td>\n",
       "      <td>651697001</td>\n",
       "      <td>2019-07-23</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>0.01</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-23</th>\n",
       "      <td>14</td>\n",
       "      <td>00007d2de826758b65a93dd24ce629ed66842531df6699...</td>\n",
       "      <td>651697001</td>\n",
       "      <td>2019-07-23</td>\n",
       "      <td>25</td>\n",
       "      <td>40</td>\n",
       "      <td>0.01</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-23</th>\n",
       "      <td>14</td>\n",
       "      <td>00007d2de826758b65a93dd24ce629ed66842531df6699...</td>\n",
       "      <td>651697001</td>\n",
       "      <td>2019-07-23</td>\n",
       "      <td>25</td>\n",
       "      <td>50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-23</th>\n",
       "      <td>14</td>\n",
       "      <td>00007d2de826758b65a93dd24ce629ed66842531df6699...</td>\n",
       "      <td>651697001</td>\n",
       "      <td>2019-07-23</td>\n",
       "      <td>50</td>\n",
       "      <td>40</td>\n",
       "      <td>0.01</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-23</th>\n",
       "      <td>14</td>\n",
       "      <td>00007d2de826758b65a93dd24ce629ed66842531df6699...</td>\n",
       "      <td>651697001</td>\n",
       "      <td>2019-07-23</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            split_id                                                       customer_id  article_id  start_test  n_factors  n_epochs  reg_all  group_id\n",
       "t_dat                                                                                                                                                 \n",
       "2019-06-23        14  00007d2de826758b65a93dd24ce629ed66842531df6699338c5570910a014cc2   651697001  2019-07-23         25        20     0.01        14\n",
       "2019-06-23        14  00007d2de826758b65a93dd24ce629ed66842531df6699338c5570910a014cc2   651697001  2019-07-23         25        40     0.01        62\n",
       "2019-06-23        14  00007d2de826758b65a93dd24ce629ed66842531df6699338c5570910a014cc2   651697001  2019-07-23         25        50     0.01        86\n",
       "2019-06-23        14  00007d2de826758b65a93dd24ce629ed66842531df6699338c5570910a014cc2   651697001  2019-07-23         50        40     0.01       158\n",
       "2019-06-23        14  00007d2de826758b65a93dd24ce629ed66842531df6699338c5570910a014cc2   651697001  2019-07-23        100        50     0.01       278"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a46e2e-743b-4096-83d9-2ead5d1d7174",
   "metadata": {
    "tags": []
   },
   "source": [
    "# surpriseSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04a81400-b881-4dff-9e0f-7b5f542b3445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# from surprise import NormalPredictor\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise import SVDpp,SVD\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import average_precision as metrics\n",
    "# import ml_metrics as metrics\n",
    "\n",
    "class surpriseSVD():\n",
    "    def __init__(self):\n",
    "        self = self\n",
    "\n",
    "    def get_top_n(self, predictions, n=12):\n",
    "        \"\"\"Return the top-N recommendation for each user from a set of predictions.\n",
    "        Args:\n",
    "            predictions(list of Prediction objects): The list of predictions, as\n",
    "                returned by the test method of an algorithm.\n",
    "            n(int): The number of recommendation to output for each user. Default\n",
    "                is 10.\n",
    "        Returns:\n",
    "        A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "            [(raw item id, rating estimation), ...] of size n.\n",
    "        \"\"\"\n",
    "\n",
    "        # First map the predictions to each user.\n",
    "        top_n = defaultdict(list)\n",
    "        for uid, iid, true_r, est, _ in predictions:\n",
    "            top_n[uid].append((iid, est))\n",
    "\n",
    "        # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "        for uid, user_ratings in top_n.items():\n",
    "            user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "            top_n[uid] = user_ratings[:n]\n",
    "\n",
    "        return top_n\n",
    "\n",
    "    def get_set(self,df):\n",
    "        reader = Reader(rating_scale=(1, 500))\n",
    "        data_set = Dataset.load_from_df(df[['customer_id','article_id','rating']], reader)\n",
    "        return data_set\n",
    "\n",
    "    def get_rating_set(self,df):\n",
    "        rating = df[['customer_id','article_id','price']].groupby(['customer_id','article_id']).count().reset_index()\n",
    "        rating.columns = ['customer_id','article_id','rating']\n",
    "        rating_set = self.get_set(rating)\n",
    "        return rating_set\n",
    "\n",
    "\n",
    "    def train_SVD(self, dataTrain, dataTest, paras={}):\n",
    "\n",
    "        ## 讀取評分資料為surprise可以訓練的格式\n",
    "        trainset = self.get_rating_set(train_data)\n",
    "        testset = self.get_rating_set(test_data)\n",
    "\n",
    "        ## rmse 需要的資料\n",
    "        testset2 = [testset.df.loc[i].to_list() for i in range(len(testset.df))]\n",
    "\n",
    "        ## map@k testing 需要產的資料\n",
    "        test_data.loc[:,'rating']=0\n",
    "        test_processed = self.get_set(test_data)\n",
    "        NA, test2 = train_test_split(test_processed, test_size=1.0)\n",
    "\n",
    "        # ======= 消費者的實際購買清單 =======\n",
    "        test_data['article_id'] = test_data['article_id'].astype('str')\n",
    "        test_uni = test_data.drop_duplicates(subset=['customer_id', 'article_id'], keep='first')\n",
    "        buy_n = test_uni[['customer_id','article_id']].groupby('customer_id')['article_id'].apply(list).to_dict()\n",
    "\n",
    "        cust_actual_list = []\n",
    "        for uid, user_ratings in buy_n.items():\n",
    "            cust_pred_tuple = (uid, [iid for iid in user_ratings])\n",
    "            cust_actual_list.append(cust_pred_tuple)\n",
    "\n",
    "        # ======= 訓練 SVD 模型 =======\n",
    "        algo = SVD(random_state=42,**paras)\n",
    "\n",
    "        # 訓練模型\n",
    "        algo.fit(trainset.build_full_trainset())\n",
    "\n",
    "        ##### rmse #####\n",
    "        predictions = algo.test(testset2)\n",
    "        rmse = accuracy.rmse(predictions)\n",
    "\n",
    "        ##### map@k #####\n",
    "        predictions_map = algo.test(test2)\n",
    "        # est = [i.est for i in predictions_map] \n",
    "\n",
    "        ##  消費者的預測清單 \n",
    "        top_n = self.get_top_n(predictions=predictions_map, n=12)\n",
    "\n",
    "        cust_pred_list = []\n",
    "        for uid, user_ratings in top_n.items():\n",
    "            cust_pred_tuple = (uid, [str(iid) for (iid, _) in user_ratings])\n",
    "            cust_pred_list.append(cust_pred_tuple)\n",
    "\n",
    "        final_list = list(zip(cust_actual_list, cust_pred_list))\n",
    "\n",
    "        # map@k計算 \n",
    "        mapk_list = []\n",
    "        for i in range(len(final_list)):\n",
    "            map_k = metrics.mapk([final_list[i][0][1]],[final_list[i][1][1]],12)\n",
    "            mapk_list.append(map_k)\n",
    "\n",
    "        map_k = sum(mapk_list)/len(mapk_list)\n",
    "\n",
    "        return rmse, map_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c6af06-e9bb-4765-98eb-eef519527caf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# pandas_udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13f0dafd-f8ce-4c20-81da-0405b2ed28b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = [True, False, False, True]\n",
    "not t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c17db0ce-2571-4c26-8588-c11677f04dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (0,1)\n",
    "b = (2,3)\n",
    "c = (4,5)\n",
    "\n",
    "x,y  = zip(a,b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec559ca9-de60-46f4-b05f-cb95ec53b8fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 5)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3870d90a-9d08-435a-9162-64ec63bd33b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train(d) -> (bool,bool):\n",
    "    train_index = (d['t_dat'] <= d['start_test'])\n",
    "    test_index = not train_index\n",
    "    return [train_index, test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "586fc2d1-126c-4d44-8048-962ce69a60eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-29 17:11:36,660 WARN scheduler.TaskSetManager: Lost task 32.0 in stage 119.0 (TID 1782) (bdse137.example.com executor 11): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/pandas/groupby.py\", line 1459, in rename_output\n",
      "  File \"/usr/local/spark/python/pyspark/pandas/frame.py\", line 2525, in apply_func\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py\", line 8833, in apply\n",
      "    return op.apply().__finalize__(self, method=\"apply\")\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/apply.py\", line 727, in apply\n",
      "    return self.apply_standard()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/apply.py\", line 851, in apply_standard\n",
      "    results, res_index = self.apply_series_generator()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/apply.py\", line 871, in apply_series_generator\n",
      "    results[i] = results[i].copy(deep=False)\n",
      "  File \"/tmp/ipykernel_17163/4018977022.py\", line 2, in get_train\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/series.py\", line 958, in __getitem__\n",
      "    return self._get_value(key)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/series.py\", line 1069, in _get_value\n",
      "    loc = self.index.get_loc(label)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py\", line 3623, in get_loc\n",
      "    raise KeyError(key) from err\n",
      "KeyError: 't_dat'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage26.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3650)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2308)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-03-29 17:11:37,482 ERROR scheduler.TaskSetManager: Task 32 in stage 119.0 failed 4 times; aborting job\n",
      "2022-03-29 17:11:37,509 WARN scheduler.TaskSetManager: Lost task 75.0 in stage 119.0 (TID 1844) (bdse108.example.com executor 4): TaskKilled (Stage cancelled)\n",
      "2022-03-29 17:11:37,518 WARN scheduler.TaskSetManager: Lost task 5.0 in stage 119.0 (TID 1757) (bdse93.example.com executor 1): TaskKilled (Stage cancelled)\n",
      "2022-03-29 17:11:37,525 WARN scheduler.TaskSetManager: Lost task 258.0 in stage 119.0 (TID 1840) (bdse106.example.com executor 10): TaskKilled (Stage cancelled)\n",
      "2022-03-29 17:11:37,526 WARN scheduler.TaskSetManager: Lost task 260.0 in stage 119.0 (TID 1842) (bdse106.example.com executor 10): TaskKilled (Stage cancelled)\n",
      "2022-03-29 17:11:37,568 WARN scheduler.TaskSetManager: Lost task 51.0 in stage 119.0 (TID 1804) (bdse137.example.com executor 11): TaskKilled (Stage cancelled)\n",
      "/usr/local/spark/python/pyspark/sql/pandas/conversion.py:153: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true, but has reached the error below and can not continue. Note that 'spark.sql.execution.arrow.pyspark.fallback.enabled' does not have an effect on failures in the middle of computation.\n",
      "  An error occurred while calling o5601.getResult.\n",
      ": org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:97)\n",
      "\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:93)\n",
      "\tat sun.reflect.GeneratedMethodAccessor65.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 32 in stage 119.0 failed 4 times, most recent failure: Lost task 32.3 in stage 119.0 (TID 1824) (bdse108.example.com executor 4): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/pandas/groupby.py\", line 1459, in rename_output\n",
      "  File \"/usr/local/spark/python/pyspark/pandas/frame.py\", line 2525, in apply_func\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py\", line 8833, in apply\n",
      "    return op.apply().__finalize__(self, method=\"apply\")\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/apply.py\", line 727, in apply\n",
      "    return self.apply_standard()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/apply.py\", line 851, in apply_standard\n",
      "    results, res_index = self.apply_series_generator()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/apply.py\", line 871, in apply_series_generator\n",
      "    results[i] = results[i].copy(deep=False)\n",
      "  File \"/tmp/ipykernel_17163/4018977022.py\", line 2, in get_train\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/series.py\", line 958, in __getitem__\n",
      "    return self._get_value(key)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/series.py\", line 1069, in _get_value\n",
      "    loc = self.index.get_loc(label)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py\", line 3623, in get_loc\n",
      "    raise KeyError(key) from err\n",
      "KeyError: 't_dat'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage26.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3650)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2308)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2309)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$5(Dataset.scala:3648)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2(Dataset.scala:3652)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2$adapted(Dataset.scala:3629)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$1(Dataset.scala:3629)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$1$adapted(Dataset.scala:3628)\n",
      "\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$2(SocketAuthServer.scala:139)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1(SocketAuthServer.scala:141)\n",
      "\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1$adapted(SocketAuthServer.scala:136)\n",
      "\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:113)\n",
      "\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:107)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$4(SocketAuthServer.scala:68)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:68)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/pandas/groupby.py\", line 1459, in rename_output\n",
      "  File \"/usr/local/spark/python/pyspark/pandas/frame.py\", line 2525, in apply_func\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py\", line 8833, in apply\n",
      "    return op.apply().__finalize__(self, method=\"apply\")\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/apply.py\", line 727, in apply\n",
      "    return self.apply_standard()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/apply.py\", line 851, in apply_standard\n",
      "    results, res_index = self.apply_series_generator()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/apply.py\", line 871, in apply_series_generator\n",
      "    results[i] = results[i].copy(deep=False)\n",
      "  File \"/tmp/ipykernel_17163/4018977022.py\", line 2, in get_train\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/series.py\", line 958, in __getitem__\n",
      "    return self._get_value(key)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/series.py\", line 1069, in _get_value\n",
      "    loc = self.index.get_loc(label)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py\", line 3623, in get_loc\n",
      "    raise KeyError(key) from err\n",
      "KeyError: 't_dat'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage26.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3650)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2308)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "  warnings.warn(msg)\n",
      "2022-03-29 17:11:37,728 WARN scheduler.TaskSetManager: Lost task 2.0 in stage 119.0 (TID 1759) (bdse91.example.com executor 6): TaskKilled (Stage cancelled)\n",
      "2022-03-29 17:11:37,728 WARN scheduler.TaskSetManager: Lost task 13.0 in stage 119.0 (TID 1768) (bdse91.example.com executor 6): TaskKilled (Stage cancelled)\n",
      "2022-03-29 17:11:37,862 WARN scheduler.TaskSetManager: Lost task 22.0 in stage 119.0 (TID 1821) (bdse90.example.com executor 8): TaskKilled (Stage cancelled)\n",
      "2022-03-29 17:11:37,863 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 119.0 (TID 1755) (bdse75.example.com executor 2): TaskKilled (Stage cancelled)\n",
      "2022-03-29 17:11:37,870 WARN scheduler.TaskSetManager: Lost task 3.0 in stage 119.0 (TID 1764) (bdse75.example.com executor 2): TaskKilled (Stage cancelled)\n",
      "2022-03-29 17:11:38,101 WARN scheduler.TaskSetManager: Lost task 7.0 in stage 119.0 (TID 1766) (bdse93.example.com executor 1): TaskKilled (Stage cancelled)\n",
      "2022-03-29 17:11:38,115 WARN scheduler.TaskSetManager: Lost task 10.1 in stage 119.0 (TID 1822) (bdse90.example.com executor 8): TaskKilled (Stage cancelled)\n",
      "2022-03-29 17:11:38,275 WARN scheduler.TaskSetManager: Lost task 11.0 in stage 119.0 (TID 1754) (bdse92.example.com executor 3): TaskKilled (Stage cancelled)\n",
      "2022-03-29 17:11:38,275 WARN scheduler.TaskSetManager: Lost task 12.0 in stage 119.0 (TID 1763) (bdse92.example.com executor 3): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o5601.getResult.\n: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:97)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:93)\n\tat sun.reflect.GeneratedMethodAccessor65.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 32 in stage 119.0 failed 4 times, most recent failure: Lost task 32.3 in stage 119.0 (TID 1824) (bdse108.example.com executor 4): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/pyspark/pandas/groupby.py\", line 1459, in rename_output\n  File \"/usr/local/spark/python/pyspark/pandas/frame.py\", line 2525, in apply_func\n  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py\", line 8833, in apply\n    return op.apply().__finalize__(self, method=\"apply\")\n  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/apply.py\", line 727, in apply\n    return self.apply_standard()\n  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/apply.py\", line 851, in apply_standard\n    results, res_index = self.apply_series_generator()\n  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/apply.py\", line 871, in apply_series_generator\n    results[i] = results[i].copy(deep=False)\n  File \"/tmp/ipykernel_17163/4018977022.py\", line 2, in get_train\n  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/series.py\", line 958, in __getitem__\n    return self._get_value(key)\n  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/series.py\", line 1069, in _get_value\n    loc = self.index.get_loc(label)\n  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py\", line 3623, in get_loc\n    raise KeyError(key) from err\nKeyError: 't_dat'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage26.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3650)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2308)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2309)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$5(Dataset.scala:3648)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2(Dataset.scala:3652)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2$adapted(Dataset.scala:3629)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$1(Dataset.scala:3629)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$1$adapted(Dataset.scala:3628)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$2(SocketAuthServer.scala:139)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1(SocketAuthServer.scala:141)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1$adapted(SocketAuthServer.scala:136)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:113)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:107)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$4(SocketAuthServer.scala:68)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:68)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/pyspark/pandas/groupby.py\", line 1459, in rename_output\n  File \"/usr/local/spark/python/pyspark/pandas/frame.py\", line 2525, in apply_func\n  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py\", line 8833, in apply\n    return op.apply().__finalize__(self, method=\"apply\")\n  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/apply.py\", line 727, in apply\n    return self.apply_standard()\n  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/apply.py\", line 851, in apply_standard\n    results, res_index = self.apply_series_generator()\n  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/apply.py\", line 871, in apply_series_generator\n    results[i] = results[i].copy(deep=False)\n  File \"/tmp/ipykernel_17163/4018977022.py\", line 2, in get_train\n  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/series.py\", line 958, in __getitem__\n    return self._get_value(key)\n  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/series.py\", line 1069, in _get_value\n    loc = self.index.get_loc(label)\n  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py\", line 3623, in get_loc\n    raise KeyError(key) from err\nKeyError: 't_dat'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage26.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3650)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2308)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [37]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_index, test_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[43msplit_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mt_dat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstart_test\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_train\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/pandas/series.py:1556\u001b[0m, in \u001b[0;36mSeries.to_list\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1544\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_list\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List:\n\u001b[1;32m   1545\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1546\u001b[0m \u001b[38;5;124;03m    Return a list of the values.\u001b[39;00m\n\u001b[1;32m   1547\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1554\u001b[0m \n\u001b[1;32m   1555\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_internal_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/pandas/series.py:6289\u001b[0m, in \u001b[0;36mSeries._to_internal_pandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   6283\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_internal_pandas\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries:\n\u001b[1;32m   6284\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   6285\u001b[0m \u001b[38;5;124;03m    Return a pandas Series directly from _internal to avoid overhead of copy.\u001b[39;00m\n\u001b[1;32m   6286\u001b[0m \n\u001b[1;32m   6287\u001b[0m \u001b[38;5;124;03m    This method is for internal use only.\u001b[39;00m\n\u001b[1;32m   6288\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 6289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_psdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pandas_frame\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname]\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/pandas/utils.py:580\u001b[0m, in \u001b[0;36mlazy_property.<locals>.wrapped_lazy_property\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_lazy_property\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    579\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr_name):\n\u001b[0;32m--> 580\u001b[0m         \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr_name, \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr_name)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/pandas/internal.py:1051\u001b[0m, in \u001b[0;36mInternalFrame.to_pandas_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;124;03m\"\"\"Return as pandas DataFrame.\"\"\"\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m sdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_internal_spark_frame\n\u001b[0;32m-> 1051\u001b[0m pdf \u001b[38;5;241m=\u001b[39m \u001b[43msdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pdf) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sdf\u001b[38;5;241m.\u001b[39mschema) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1053\u001b[0m     pdf \u001b[38;5;241m=\u001b[39m pdf\u001b[38;5;241m.\u001b[39mastype(\n\u001b[1;32m   1054\u001b[0m         {field\u001b[38;5;241m.\u001b[39mname: spark_type_to_pandas_dtype(field\u001b[38;5;241m.\u001b[39mdataType) \u001b[38;5;28;01mfor\u001b[39;00m field \u001b[38;5;129;01min\u001b[39;00m sdf\u001b[38;5;241m.\u001b[39mschema}\n\u001b[1;32m   1055\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/pandas/conversion.py:109\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m tmp_column_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcol_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns))]\n\u001b[1;32m    108\u001b[0m self_destruct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msql_ctx\u001b[38;5;241m.\u001b[39m_conf\u001b[38;5;241m.\u001b[39marrowPySparkSelfDestructEnabled()\n\u001b[0;32m--> 109\u001b[0m batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoDF\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtmp_column_names\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collect_as_arrow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_batches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_destruct\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batches) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    112\u001b[0m     table \u001b[38;5;241m=\u001b[39m pyarrow\u001b[38;5;241m.\u001b[39mTable\u001b[38;5;241m.\u001b[39mfrom_batches(batches)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/pandas/conversion.py:286\u001b[0m, in \u001b[0;36mPandasConversionMixin._collect_as_arrow\u001b[0;34m(self, split_batches)\u001b[0m\n\u001b[1;32m    283\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(batch_stream)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Join serving thread and raise any exceptions from collectAsArrowToPython\u001b[39;00m\n\u001b[0;32m--> 286\u001b[0m     \u001b[43mjsocket_auth_server\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# Separate RecordBatches from batch order indices in results\u001b[39;00m\n\u001b[1;32m    289\u001b[0m batches \u001b[38;5;241m=\u001b[39m results[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o5601.getResult.\n: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:97)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:93)\n\tat sun.reflect.GeneratedMethodAccessor65.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 32 in stage 119.0 failed 4 times, most recent failure: Lost task 32.3 in stage 119.0 (TID 1824) (bdse108.example.com executor 4): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/pyspark/pandas/groupby.py\", line 1459, in rename_output\n  File \"/usr/local/spark/python/pyspark/pandas/frame.py\", line 2525, in apply_func\n  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py\", line 8833, in apply\n    return op.apply().__finalize__(self, method=\"apply\")\n  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/apply.py\", line 727, in apply\n    return self.apply_standard()\n  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/apply.py\", line 851, in apply_standard\n    results, res_index = self.apply_series_generator()\n  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/apply.py\", line 871, in apply_series_generator\n    results[i] = results[i].copy(deep=False)\n  File \"/tmp/ipykernel_17163/4018977022.py\", line 2, in get_train\n  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/series.py\", line 958, in __getitem__\n    return self._get_value(key)\n  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/series.py\", line 1069, in _get_value\n    loc = self.index.get_loc(label)\n  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py\", line 3623, in get_loc\n    raise KeyError(key) from err\nKeyError: 't_dat'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage26.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3650)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2308)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2309)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$5(Dataset.scala:3648)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2(Dataset.scala:3652)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2$adapted(Dataset.scala:3629)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$1(Dataset.scala:3629)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$1$adapted(Dataset.scala:3628)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$2(SocketAuthServer.scala:139)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1(SocketAuthServer.scala:141)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1$adapted(SocketAuthServer.scala:136)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:113)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:107)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$4(SocketAuthServer.scala:68)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:68)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/pyspark/pandas/groupby.py\", line 1459, in rename_output\n  File \"/usr/local/spark/python/pyspark/pandas/frame.py\", line 2525, in apply_func\n  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py\", line 8833, in apply\n    return op.apply().__finalize__(self, method=\"apply\")\n  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/apply.py\", line 727, in apply\n    return self.apply_standard()\n  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/apply.py\", line 851, in apply_standard\n    results, res_index = self.apply_series_generator()\n  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/apply.py\", line 871, in apply_series_generator\n    results[i] = results[i].copy(deep=False)\n  File \"/tmp/ipykernel_17163/4018977022.py\", line 2, in get_train\n  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/series.py\", line 958, in __getitem__\n    return self._get_value(key)\n  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/series.py\", line 1069, in _get_value\n    loc = self.index.get_loc(label)\n  File \"/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py\", line 3623, in get_loc\n    raise KeyError(key) from err\nKeyError: 't_dat'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage26.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.hasNext(ArrowConverters.scala:99)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.foreach(ArrowConverters.scala:97)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.to(ArrowConverters.scala:97)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toBuffer(ArrowConverters.scala:97)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.toArray(ArrowConverters.scala:97)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3650)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2308)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "train_index, test_index = zip(split_data[['t_dat','start_test']].apply(get_train).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bace2342-f05b-443b-bd35-b37c2cc3e0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_split_hyperparameter_search(data):\n",
    "    paras = {\n",
    "        'factors':data.factors.values[0], \n",
    "        'iterations':dta.iterations.values[0], \n",
    "        'regularization':data.regularization.values[0]\n",
    "    }\n",
    "    \n",
    "    train_index, test_index = join_data.apply(get_train, axis=1)\n",
    "    dataTrain = data[(data['t_dat'] <= data['start_test'])]\n",
    "    dataTest = data[(data['t_dat'] > start_test)]\n",
    "    \n",
    "    rmse, map12 = model.train_SVD(dataTrain, dataTest, paras)\n",
    "    \n",
    "    paras.update({\n",
    "        'date_x_paras_id' : data.date_x_paras_id.values[0],\n",
    "        'val_date' : data.val_date.values[0],\n",
    "        'map12' : map12\n",
    "    })\n",
    "    \n",
    "    results = pd.DataFrame([paras])\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3260e8-a8f7-4ea0-9ef1-fab3934b9a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas_udf\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField('date_x_paras_id', IntegerType(),True),\n",
    "        StructField('map12', FloatType(),True),\n",
    "        StructField('val_date', DateTime(),True),\n",
    "        StructField(\"para1\", IntegerType(), True),\n",
    "        StructField(\"para2\", IntegerType(), True)\n",
    "     ]\n",
    ")\n",
    "\n",
    "results = df.groupby('date_x_paras_id').applyInPandas(time_split_hyperparameter_search, schema)\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5348c4-3990-44eb-84b9-2eed404cc61b",
   "metadata": {},
   "source": [
    "# (DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b996c3-1b5e-4f11-8776-c5a589f457b2",
   "metadata": {},
   "source": [
    "## 1. 讀取檔案 /DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1431f87d-52d0-4c07-9609-ac6e977ea0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 讀取檔案\n",
    "# customers = spark.read.option('header','true').parquet('/user/HM_parquet/customers.parquet')\n",
    "# articles = spark.read.option('header','true').parquet('/user/HM_parquet/articles.parquet')\n",
    "# transactions = spark.read.option('header','true').parquet('/user/HM_parquet/transactions_train.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a841413-2d9a-4f01-ba53-5d301b39082d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transactions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1012b563-2eb7-40c8-8d2a-39a119f9e288",
   "metadata": {},
   "source": [
    "## 2. 將customer_id(字串)轉為customer_index(整數) /DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634903ba-7100-4a88-9f24-7becc2e1c40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 將customers的customer_id轉為數字(buffer要增加到512m)\n",
    "# toIndex = StringIndexer(inputCol=\"customer_id\", outputCol=\"customer_index\").fit(customers)\n",
    "# customers = toIndex.transform(customers)\n",
    "# # customers.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2875dbbe-f263-4550-bca3-79c887a4901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 將transactions的customer_id轉為數字\n",
    "# transactions = toIndex.transform(transactions)\n",
    "# # transactions.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf64ded3-e050-46e5-a4c2-cebf60cb422d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transactions.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
